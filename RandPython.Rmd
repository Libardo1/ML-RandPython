---
title: "RandPython"
author: "Tinniam V Ganesh"
date: "October 3, 2017"
output:
  html_document: default
  word_document: default
---
## Univariate Regression - R code



```{r}
source("RFunctions.R")

df=read.csv("Boston.csv",stringsAsFactors = FALSE) # Data from MASS - SL
train_idx <- trainTestSplit(df,trainPercent=75,seed=5)
train <- df[train_idx, ]
test <- df[-train_idx, ]


fit=lm(medv~lstat,data=df)
fit
summary(fit)
confint(fit)
plot(df$lstat,df$medv)
abline(fit)
abline(fit,lwd=3)
abline(fit,lwd=3,col="red")
```


## Univariate Regression - Python code

## R code
```{python}
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
os.chdir("C:\\software\\machine-learning\\RandPython")
df = pd.read_csv("Boston.csv",encoding = "ISO-8859-1")
X=df['lstat']
y=df['medv']
X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0)
X_train=X_train.values.reshape(-1,1)
X_test=X_test.values.reshape(-1,1)


linreg = LinearRegression().fit(X_train, y_train)
print('linear model coeff (w): {}'
     .format(linreg.coef_))
print('linear model intercept (b): {:.3f}'
     .format(linreg.intercept_))
print('R-squared score (training): {:.3f}'
     .format(linreg.score(X_train, y_train)))
print('R-squared score (test): {:.3f}'
     .format(linreg.score(X_test, y_test)))
     

fig=plt.scatter(X_train,y_train)
plt.plot()
# Create a range of points. Compute yhat=coeff1*x + intercept and plot
x=np.linspace(0,40,20)
plt.plot(x, linreg.coef_ * x + linreg.intercept_, color='red')
#plt.show()
fig.figure.savefig('foo.png', bbox_inches='tight')
print "finished"

```


Output image:
![output](foo.png)

## Multivariate Regression - R code


```{r a,message=FALSE, warning=FALSE}
crimesDF <- read.csv("crimes.csv",stringsAsFactors = FALSE)

crimesDF1 <- crimesDF[,7:length(crimesDF)]

# Conert all to numeric
crimesDF2 <- sapply(crimesDF1,as.numeric)

# Check for NAs
a <- is.na(crimesDF2)
# Set to 0 as an imputation
crimesDF2[a] <-0
crimesDF2 <- as.data.frame(crimesDF2)
train_idx <- trainTestSplit(crimesDF2,trainPercent=75,seed=5)
train <- crimesDF2[train_idx, ]
test <- crimesDF2[-train_idx, ]


fit <- lm(ViolentCrimesPerPop~.,data=train)
rsquared=Rsquared(fit,test,test$ViolentCrimesPerPop)
sprintf("R-squared for multi-variate regression (crimes.csv)  is : %f", rsquared)
```

## Multivariate Regression - R code
```{python}
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
crimesDF =pd.read_csv("crimes.csv",encoding="ISO-8859-1")
crimesDF1=crimesDF.iloc[:,7:crimesDF.shape[1]]
crimesDF2 = crimesDF1.apply(pd.to_numeric, errors='coerce')
crimesDF2.fillna(0, inplace=True)
X=crimesDF2.iloc[:,0:120]
y=crimesDF2.iloc[:,121]

X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0)
linreg = LinearRegression().fit(X_train, y_train)

print('R-squared score (training): {:.3f}'
     .format(linreg.score(X_train, y_train)))
print('R-squared score (test): {:.3f}'
     .format(linreg.score(X_test, y_test)))
```

# Polynomial Regression - R 
Using polynomials of degree 1,2 & 3
```{r,message=FALSE, warning=FALSE}
 # Polynommial depgree 1
df=read.csv("auto_mpg.csv",stringsAsFactors = FALSE) # Data from UCI
df1 <- as.data.frame(sapply(df,as.numeric))

df2 <- df1 %>% select(cylinder,displacement, horsepower,weight, acceleration, year,mpg)
df3 <- df2[complete.cases(df2),]
train_idx <- trainTestSplit(df3,trainPercent=75,seed=5)
train <- df3[train_idx, ]
test <- df3[-train_idx, ]
# Fit a model of degree 1
fit <- lm(mpg~. ,data=train)
rsquared1 <-Rsquared(fit,test,test$mpg)
sprintf("R-squared for Polynomial regression of degree 1 (auto_mpg.csv)  is : %f", rsquared1)

#Polynomial degree 2
x = as.matrix(df3[1:6])
# Make poly of total Data frame before split
df4=as.data.frame(poly(x,2,raw=TRUE))
df5 <- cbind(df4,df3[7])
train_idx <- trainTestSplit(df5,trainPercent=75,seed=5)
train <- df5[train_idx, ]
test <- df5[-train_idx, ]
# Fit the model
fit <- lm(mpg~. ,data=train)
rsquared2=Rsquared(fit,test,test$mpg)
sprintf("R-squared for Polynomial regression of degree 2 (auto_mpg.csv)  is : %f", rsquared2)


#Polynomial degree 3
x = as.matrix(df3[1:6])
# Make poly of total Data frame before split
df4=as.data.frame(poly(x,3,raw=TRUE))
df5 <- cbind(df4,df3[7])
train_idx <- trainTestSplit(df5,trainPercent=75,seed=5)
train <- df5[train_idx, ]
test <- df5[-train_idx, ]
# Fit the model
fit <- lm(mpg~. ,data=train)
rsquared3=Rsquared(fit,test,test$mpg)
sprintf("R-squared for Polynomial regression of degree 2 (auto_mpg.csv)  is : %f", rsquared3)

df=data.frame(degree=c(1,2,3),Rsquared=c(rsquared1,rsquared2,rsquared3))

ggplot(df,aes(x=degree,y=Rsquared)) +geom_point() + geom_line(color="blue") +
    ggtitle("Polynomial regression - R squared vs Degree of polynomial") +
    xlab("Degree") + ylab("R squared")
```


```{python}
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
autoDF =pd.read_csv("auto_mpg.csv",encoding="ISO-8859-1")
autoDF.shape
autoDF.columns
autoDF1=autoDF[['mpg','cylinder','displacement','horsepower','weight','acceleration','year']]
autoDF2 = autoDF1.apply(pd.to_numeric, errors='coerce')
autoDF3=autoDF2.dropna()
autoDF3.shape
X=autoDF3[['cylinder','displacement','horsepower','weight','acceleration','year']]
y=autoDF3['mpg']

# Polynomial degree 1
X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0)
linreg = LinearRegression().fit(X_train, y_train)
print('R-squared score - Polynomial degree 1 (training): {:.3f}'
     .format(linreg.score(X_train, y_train)))
     
rsquared1 =linreg.score(X_test, y_test)
print('R-squared score - Polynomial degree 1 (test): {:.3f}'
     .format(linreg.score(X_test, y_test)))

# Polynomial degree 2
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y,random_state = 0)
linreg = LinearRegression().fit(X_train, y_train)

print('R-squared score - Polynomial degree 2 (training): {:.3f}'
     .format(linreg.score(X_train, y_train)))

rsquared2 =linreg.score(X_test, y_test)
print('R-squared score - Polynomial degree 2 (test): {:.3f}\n'
     .format(linreg.score(X_test, y_test)))

#Polynomial degree 3

poly = PolynomialFeatures(degree=3)
X_poly = poly.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y,random_state = 0)
linreg = LinearRegression().fit(X_train, y_train)
print('(R-squared score -Polynomial degree 3  (training): {:.3f}'
     .format(linreg.score(X_train, y_train)))
     
rsquared3 =linreg.score(X_test, y_test)
print('R-squared score Polynomial degree 3 (test): {:.3f}\n'
     .format(linreg.score(X_test, y_test)))
degree=[1,2,3]
rsquared =[rsquared1,rsquared2,rsquared3]
fig=plt.plot(degree,rsquared)
fig=plt.title("Polynomial regression - R squared vs Degree of polynomial")
fig=plt.xlabel("Degree")
fig=plt.ylabel("R squared")
fig.figure.savefig('foo.png', bbox_inches='tight')
print "Finished plotting and saving"


```


Output image:
![output](foo.png)
```



```{r}
# knn.reg requires the FNN package
df=read.csv("auto_mpg.csv",stringsAsFactors = FALSE) # Data from UCI
df1 <- as.data.frame(sapply(df,as.numeric))

df2 <- df1 %>% select(cylinder,displacement, horsepower,weight, acceleration, year,mpg)
df3 <- df2[complete.cases(df2),]

train_idx <- trainTestSplit(df3,trainPercent=75,seed=5)
train <- df3[train_idx, ]
test <- df3[-train_idx, ]
# Fir the 
train.X=train[,1:6]
train.Y=train[,7]
test.X=test[,1:6]
test.Y=test[,7]
for(i in c(1,2,4,8,10,14,20)){
    knn=knn.reg(train.X,test.X,train.Y,k=i)
    a=knnRSquared(knn$pred,test.Y)
    print(a)
    
}
```


```{python}
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.neighbors import KNeighborsRegressor
autoDF =pd.read_csv("auto_mpg.csv",encoding="ISO-8859-1")
autoDF.shape
autoDF.columns
autoDF1=autoDF[['mpg','cylinder','displacement','horsepower','weight','acceleration','year']]
autoDF2 = autoDF1.apply(pd.to_numeric, errors='coerce')
autoDF3=autoDF2.dropna()
autoDF3.shape
X=autoDF3[['cylinder','displacement','horsepower','weight','acceleration','year']]
y=autoDF3['mpg']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)

for neighbors in [1,2,4,8,10,14,20]:
     knnreg = KNeighborsRegressor(n_neighbors = neighbors).fit(X_train, y_train)
     #print(knnreg.predict(X_test))
     print('R-squared test score: {:.3f}'
        .format(knnreg.score(X_test, y_test)))
```


```{r}
df=read.csv("auto_mpg.csv",stringsAsFactors = FALSE) # Data from UCI
df1 <- as.data.frame(sapply(df,as.numeric))

df2 <- df1 %>% select(cylinder,displacement, horsepower,weight, acceleration, year,mpg)
df3 <- df2[complete.cases(df2),]
train.X.scaled=MinMaxScaler(train.X)
test.X.scaled=MinMaxScaler(test.X)

knn=knn.reg(train.X.scaled,test.X.scaled,train.Y,k=11)
knnRSquared(knn$pred,test.Y)
for(i in c(1,2,4,6,8,10,12,15,20,25,30)){
    knn=knn.reg(train.X.scaled,test.X.scaled,train.Y,k=i)
    a=knnRSquared(knn$pred,test.Y)
    print(a)
}
```


```{python}
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import MinMaxScaler
autoDF =pd.read_csv("auto_mpg.csv",encoding="ISO-8859-1")
autoDF.shape
autoDF.columns
autoDF1=autoDF[['mpg','cylinder','displacement','horsepower','weight','acceleration','year']]
autoDF2 = autoDF1.apply(pd.to_numeric, errors='coerce')
autoDF3=autoDF2.dropna()
autoDF3.shape
X=autoDF3[['cylinder','displacement','horsepower','weight','acceleration','year']]
y=autoDF3['mpg']


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
# we must apply the scaling to the test set that we computed for the training set
X_test_scaled = scaler.transform(X_test)

for neighbors in [1,2,4,6,8,10,12,15,20,25,30]:
    knnreg = KNeighborsRegressor(n_neighbors = neighbors).fit(X_train_scaled, y_train)

    print('R-squared test score: {:.3f}'
        .format(knnreg.score(X_test_scaled, y_test)))
```

